{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# *Merged Jupyter Notebook*"]}, {"cell_type": "markdown", "metadata": {}, "source": "<hr><font color=\"green\"><h1>from file: 02_07_Decision_Trees</h1></font>"}, {"cell_type": "markdown", "metadata": {}, "source": ["One of the most important considerations when choosing a machine learning algorithm is how interpretable it is. The ability to explain how an algorithm makes predictions is useful to not only you, but also to potential stakeholders. A very interpretable machine learning algorithm is a decision tree which you can think of as a series of questions designed to assign a class or predict a continuous value depending on the task. The example image is a decision tree designed for classification.\n", "\n", "![image](images/decisionTreeExample.png)\n", "\n", "In this video, I'll share with you how you can create and tune a decision tree using scikit-learn. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Import Libraries"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "\n", "from sklearn.datasets import load_iris\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.tree import DecisionTreeClassifier"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["## Load the Dataset\n", "The Iris dataset is one of datasets scikit-learn comes with that do not require the downloading of any file from some external website. The code below loads the iris dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = load_iris()\n", "df = pd.DataFrame(data.data, columns=data.feature_names)\n", "df['target'] = data.target\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Splitting Data into Training and Test Sets"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![images](images/trainTestSplit.png)\n", "The colors in the image indicate which variable (X_train, X_test, Y_train, Y_test) the data from the dataframe df went to for a particular train test split (not necessarily the exact split of the code below)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(df[data.feature_names], df['target'], random_state=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note, another benefit of Decision Trees is that you don\u00e2\u20ac\u2122t have to standardize your features unlike other algorithms like logistic regression and K-Nearest Neighbors. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Decision Tree\n", "\n", "<b>Step 1:</b> Import the model you want to use\n", "\n", "In sklearn, all machine learning models are implemented as Python classes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# This was already imported earlier in the notebook so commenting out\n", "#from sklearn.tree import DecisionTreeClassifier"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<b>Step 2:</b> Make an instance of the Model\n", "\n", "This is a place where we can tune the hyperparameters of a model. The code below constrains the model to have at most a depth of 2. Tree depth is a measure of how many splits it makes before coming to a prediction."]}, {"cell_type": "markdown", "metadata": {}, "source": ["![images](images/max_depth_not_depth.png)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf = DecisionTreeClassifier(max_depth = 2, \n", "                             random_state = 0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<b>Step 3:</b> Training the model on the data, storing the information learned from the data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Model is learning the relationship between x (features sepal width, sepal height etc) and y (labels-which species of iris)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<b>Step 4:</b> Predict the labels of new data (new flowers)\n", "\n", "Uses the information the model learned during the model training process"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Predict for One Observation\n", "clf.predict(X_test.iloc[0].values.reshape(1, -1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Predict for Multiple Observations at Once"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf.predict(X_test[0:10])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Measuring Model Performance"]}, {"cell_type": "markdown", "metadata": {}, "source": ["While there are other ways of measuring model performance (precision, recall, F1 Score, [ROC Curve](https://towardsdatascience.com/receiver-operating-characteristic-curves-demystified-in-python-bd531a4364d0), etc), we are going to keep this simple and use accuracy as our metric.\u00c2\u00a0\n", "To do this are going to see how the model performs on new data (test set)\n", "\n", "Accuracy is defined as:\n", "(fraction of correct predictions): correct predictions / total number of data points"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["score = clf.score(X_test, y_test)\n", "print(score)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Finding the Optimal `max_depth`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# List of values to try for max_depth:\n", "max_depth_range = list(range(1, 6))\n", "\n", "# List to store the average RMSE for each value of max_depth:\n", "accuracy = []\n", "\n", "for depth in max_depth_range:\n", "    \n", "    clf = DecisionTreeClassifier(max_depth = depth, \n", "                             random_state = 0)\n", "    clf.fit(X_train, y_train)\n", "\n", "    score = clf.score(X_test, y_test)\n", "    accuracy.append(score)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (10,7));\n", "\n", "ax.plot(max_depth_range,\n", "        accuracy,\n", "        lw=2,\n", "        color='k')\n", "\n", "ax.set_xlim([1, 5])\n", "ax.set_ylim([.50, 1.00])\n", "ax.grid(True,\n", "        axis = 'both',\n", "        zorder = 0,\n", "        linestyle = ':',\n", "        color = 'k')\n", "\n", "yticks = ax.get_yticks()\n", "\n", "y_ticklist = []\n", "for tick in yticks:\n", "    y_ticklist.append(str(tick).ljust(4, '0')[0:4])\n", "ax.set_yticklabels(y_ticklist)\n", "ax.tick_params(labelsize = 18)\n", "ax.set_xticks([1,2,3,4,5])\n", "ax.set_xlabel('max_depth', fontsize = 24)\n", "ax.set_ylabel('Accuracy', fontsize = 24)\n", "fig.tight_layout()\n", "#fig.savefig('images/max_depth_vs_accuracy.png', dpi = 300)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["So that's it, I encourage you to try and see if you can create a decision tree of your own. "]}, {"cell_type": "markdown", "metadata": {}, "source": "<hr><font color=\"green\"><h1>from file: 02_08_How_to_Visualize_Decision_Trees</h1></font>"}, {"cell_type": "markdown", "metadata": {}, "source": ["How do you understand how a decision tree makes predictions?\n", "One of the strengths of decision trees are that they are relatively easy to interpret as you can make a visualization based on your model. This is not only a powerful way to understand your model, but also to communicate how your model works to stakeholders. \n", "\n", "\n", "In this video, I'll show you how Decision Trees can be plotted with Matplotlib."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Import Libraries"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "\n", "from sklearn.datasets import load_iris\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.tree import DecisionTreeClassifier\n", "\n", "from sklearn import tree"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["## Load the Dataset\n", "The Iris dataset is one of datasets scikit-learn comes with that do not require the downloading of any file from some external website. The code below loads the iris dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = load_iris()\n", "df = pd.DataFrame(data.data, columns=data.feature_names)\n", "df['target'] = data.target\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Split Data into Training and Test Sets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, Y_train, Y_test = train_test_split(df[data.feature_names], df['target'], random_state=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Scikit-learn 4-Step Modeling\u00c2\u00a0Pattern\n", "\n", "<b>Step 1:</b> Import the model you want to use\n", "\n", "In sklearn, all machine learning models are implemented as Python classes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# This was already imported earlier in the notebook so commenting out\n", "#from sklearn.tree import DecisionTreeClassifier"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<b>Step 2:</b> Make an instance of the Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf = DecisionTreeClassifier(max_depth = 2, \n", "                             random_state = 0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<b>Step 3:</b> Training the model on the data, storing the information learned from the data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Model is learning the relationship between x (features: sepal width, sepal height etc) and y (labels-which species of iris)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf.fit(X_train, Y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<b>Step 4:</b> Predict the labels of new data (new flowers)\n", "\n", "Uses the information the model learned during the model training process"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Predict for One Observation (image)\n", "clf.predict(X_test.iloc[0].values.reshape(1, -1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Predict for Multiple Observations (images) at Once"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf.predict(X_test[0:10])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Measuring Model Performance"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Accuracy is defined as:\n", "(fraction of correct predictions): correct predictions / total number of data points"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["score = clf.score(X_test, Y_test)\n", "print(score)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## How to Visualize Decision Trees using Matplotlib"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Default Visualization Based on the Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tree.plot_tree(clf);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Adjust Figure Size and Dots per inch (DPI)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (4,4), dpi = 300)\n", "\n", "tree.plot_tree(clf);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Make Tree More Interpretable\n", "The code below not only allows you to save a visualization based on your model, but also makes the decision tree more interpretable by adding in feature and class names."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Putting the feature names and class names into variables\n", "fn = ['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']\n", "cn = ['setosa', 'versicolor', 'virginica']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (4,4), dpi = 300)\n", "\n", "tree.plot_tree(clf,\n", "               feature_names = fn, \n", "               class_names=cn,\n", "               filled = True);\n", "fig.savefig('images/plottreefncn.png')"]}, {"cell_type": "markdown", "metadata": {}, "source": "<hr><font color=\"green\"><h1>from file: 02_09_Bagged_Trees</h1></font>"}, {"cell_type": "markdown", "metadata": {}, "source": ["Each machine learning algorithm has strengths and weaknesses. A weakness of decision trees is that they are prone to overfitting on the training set. A way to mitigate this problem is to constrain how large a tree can grow. Bagged trees try to overcome this weakness by using bootstrapped data to grow multiple deep decision trees. The idea is that many trees protect each other from individual weaknesses.\n", "![images](images/baggedTrees.png)\n", "\n", "In this video, I'll share with you how you can build a bagged tree model for regression."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Import Libraries"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "import numpy as np\n", "\n", "from sklearn.model_selection import train_test_split\n", "\n", "# Bagged Trees Regressor\n", "from sklearn.ensemble import BaggingRegressor"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["## Load the Dataset\n", "This dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015. The code below loads the dataset. The goal of this dataset is to predict price based on features like number of bedrooms and bathrooms"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('data/kc_house_data.csv')\n", "\n", "df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# This notebook only selects a couple features for simplicity\n", "# However, I encourage you to play with adding and substracting more features\n", "features = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors']\n", "\n", "X = df.loc[:, features]\n", "\n", "y = df.loc[:, 'price'].values"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Splitting Data into Training and Test Sets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note, another benefit of bagged trees like decision trees is that you don\u00e2\u20ac\u2122t have to standardize your features unlike other algorithms like logistic regression and K-Nearest Neighbors. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Bagged Trees\n", "\n", "<b>Step 1:</b> Import the model you want to use\n", "\n", "In sklearn, all machine learning models are implemented as Python classes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# This was already imported earlier in the notebook so commenting out\n", "#from sklearn.ensemble import BaggingRegressor"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<b>Step 2:</b> Make an instance of the Model\n", "\n", "This is a place where we can tune the hyperparameters of a model. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["reg = BaggingRegressor(n_estimators=100, \n", "                       random_state = 0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<b>Step 3:</b> Training the model on the data, storing the information learned from the data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Model is learning the relationship between X (features like number of bedrooms) and y (price)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["reg.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<b>Step 4:</b> Make Predictions\n", "\n", "Uses the information the model learned during the model training process"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Returns a NumPy Array\n", "# Predict for One Observation\n", "reg.predict(X_test.iloc[0].values.reshape(1, -1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Predict for Multiple Observations at Once"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["reg.predict(X_test[0:10])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Measuring Model Performance"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Unlike classification models where a common metric is accuracy, regression models use other metrics like R^2, the coefficient of determination to quantify your model's performance. The best possible score is 1.0. A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["score = reg.score(X_test, y_test)\n", "print(score)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Tuning n_estimators (Number of Decision Trees)\n", "\n", "A tuning parameter for bagged trees is **n_estimators**, which represents the number of trees that should be grown. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# List of values to try for n_estimators:\n", "estimator_range = [1] + list(range(10, 150, 20))\n", "\n", "scores = []\n", "\n", "for estimator in estimator_range:\n", "    reg = BaggingRegressor(n_estimators=estimator, random_state=0)\n", "    reg.fit(X_train, y_train)\n", "    scores.append(reg.score(X_test, y_test))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize = (10,7))\n", "plt.plot(estimator_range, scores);\n", "\n", "plt.xlabel('n_estimators', fontsize =20);\n", "plt.ylabel('Score', fontsize = 20);\n", "plt.tick_params(labelsize = 18)\n", "plt.grid()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice that the score stops improving after a certain number of estimators (decision trees). One way to get a better score would be to include more features in the features matrix. So that's it, I encourage you to try a building a bagged tree model "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 2}